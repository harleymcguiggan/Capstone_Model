# -*- coding: utf-8 -*-
"""MLInvestor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GF4wFrg7BGvoJ_wurWLHKyWEZFZWX_rs
"""

from google.colab import drive
 drive.mount('/content/drive')

!pip install --upgrade mplfinance

# Basic libraries
import numpy as np
import pandas as pd
import warnings
import tensorflow as tf


warnings.filterwarnings("ignore")

# For processing
import math
import random
import datetime as dt
import matplotlib.dates as mdates

# For visualization
import matplotlib.pyplot as plt
from mplfinance.original_flavor import candlestick_ohlc

# Libraries for model training
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.metrics import mean_squared_error

# Reading our datasets
datasetAAPL = pd.read_csv('/content/drive/MyDrive/DATA/AAPL.csv', parse_dates=['Date'], dayfirst=True)
datasetAAPL.head()

datasetAMD = pd.read_csv('/content/drive/MyDrive/DATA/AMD.csv', parse_dates=['Date'], dayfirst=True)
datasetAMD.head()

datasetGOOGL = pd.read_csv('/content/drive/MyDrive/DATA/GOOGL.csv', parse_dates=['Date'], dayfirst=True)
datasetGOOGL.head()

datasetNVDA = pd.read_csv('/content/drive/MyDrive/DATA/NVDA.csv', parse_dates=['Date'], dayfirst=True)
datasetNVDA.head()

datasetTEAM = pd.read_csv('/content/drive/MyDrive/DATA/TEAM.csv', parse_dates=['Date'], dayfirst=True)
datasetTEAM.head()

# Line Chart of Closing Prices Over Time

# Define a function to plot closing prices
def plot_closing_prices(dataset, title):
    dataset['Date'] = pd.to_datetime(dataset['Date'])

    plt.figure(figsize=(15, 6))
    plt.plot(dataset['Date'], dataset['Close'], marker='.')
    plt.title(f'Close Price Over Time - {title}')
    plt.xlabel('Date')
    plt.ylabel('Closing Price')
    plt.xticks(rotation=45)
    plt.grid(True)
    plt.show()

# Example usage for multiple datasets
datasets = [(datasetAAPL, 'AAPL'), (datasetAMD, 'AMD'), (datasetGOOGL, 'GOOGL'), (datasetNVDA, 'NVDA'), (datasetTEAM, 'TEAM')]

for data, title in datasets:
    plot_closing_prices(data, title)

# # Candlestick

# Define a function to plot candlestick charts
def plot_candlestick(dataset, title):
    # Convert 'Date' column to matplotlib date format
    matplotlib_date = mdates.date2num(dataset['Date'])

    # Create an array of tuples in the required format
    ohlc = np.vstack((matplotlib_date, dataset['Open'], dataset['High'], dataset['Low'], dataset['Close'])).T

    # Create a new figure and subplot
    plt.figure(figsize=(15, 6))
    ax = plt.subplot()

    # Plot the candlestick chart
    candlestick_ohlc(ax, ohlc, width=0.6, colorup='g', colordown='r')

    ax.xaxis_date()
    plt.title(f'Candlestick Chart - {title}')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.xticks(rotation=45)
    plt.grid(True)
    plt.show()

# Example usage for multiple datasets
datasets = [(datasetAAPL, 'AAPL'), (datasetAMD, 'AMD'), (datasetGOOGL, 'GOOGL'), (datasetNVDA, 'NVDA'),(datasetTEAM, 'TEAM')]

for data, title in datasets:
    plot_candlestick(data, title)

# Plotting of Closing Price against 30-Day Moving Average (30DMA)

# Define function to plot Closing Price against 30DMA
def plot_closing_price_and_moving_average(dataset, symbol, window=30):
    plt.figure(figsize=(15, 6))
    plt.plot(dataset['Date'], dataset['Close'], label='Closing Price', linewidth=2)
    plt.plot(dataset['Date'], dataset['Close'].rolling(window=window).mean(), label=f'{window}-Day Moving Avg', linestyle='--')
    plt.title(f'Closing Prices and {window}-Day Moving Average - {symbol}')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.xticks(rotation=45)
    plt.legend()
    plt.grid(True)
    plt.show()

# Example usage for multiple datasets
datasets = [(datasetAAPL, 'AAPL'), (datasetAMD, 'AMD'), (datasetGOOGL, 'GOOGL'), (datasetNVDA, 'NVDA'), (datasetTEAM, 'TEAM')]

for data, symbol in datasets:
    plot_closing_price_and_moving_average(data, symbol)

display(datasetAAPL.head())
display(datasetAMD.head())
display(datasetGOOGL.head())
display(datasetNVDA.head())
display(datasetTEAM.head())

def preprocess_dataset(dataset):
    return dataset.reset_index()['Close']

new_datasetAAPL = preprocess_dataset(datasetAAPL)
new_datasetAMD = preprocess_dataset(datasetAMD)
new_datasetGOOGL = preprocess_dataset(datasetGOOGL)
new_datasetNVDA = preprocess_dataset(datasetNVDA)
new_datasetTEAM = preprocess_dataset(datasetTEAM)

# Normalizing our data using MinMaxScaler
scaler = MinMaxScaler()
def scale_dataset(dataset):

    scaled_dataset = scaler.fit_transform(np.array(dataset).reshape(-1, 1))
    return scaled_dataset

scaled_datasetAAPL = scale_dataset(new_datasetAAPL)
scaled_datasetAMD = scale_dataset(new_datasetAMD)
scaled_datasetGOOGL = scale_dataset(new_datasetGOOGL)
scaled_datasetNVDA = scale_dataset(new_datasetNVDA)
scaled_datasetTEAM = scale_dataset(new_datasetTEAM)

# Split into training and testing sets
def split_dataset(dataset, split_ratio=0.8):
    train_size = int(len(dataset) * split_ratio)
    train_data, test_data = dataset[:train_size], dataset[train_size:]
    return train_data, test_data

train_data_AAPL, test_data_AAPL = split_dataset(datasetAAPL)
train_data_AMD, test_data_AMD = split_dataset(datasetAMD)
train_data_GOOGL, test_data_GOOGL = split_dataset(datasetGOOGL)
train_data_NVDA, test_data_NVDA = split_dataset(datasetNVDA)
train_data_TEAM, test_data_TEAM = split_dataset(datasetTEAM)

def prepare_sequences(dataset, n_past, target_col='Close'):
    X, y = [], []

    # Check if the dataset has enough data points
    if len(dataset) >= n_past:
        for i in range(n_past, len(dataset)):
            X.append(dataset[target_col].iloc[i - n_past:i].values)
            y.append(dataset[target_col].iloc[i])
        X, y = np.array(X), np.array(y)
        return X, y
    else:
        raise ValueError("Dataset does not have enough data points for the specified sequence length.")



n_past = 60  # Define the sequence length (number of past time steps)

# Assuming your datasets are Pandas DataFrames, modify the following lines accordingly
# For example: train_data_AAPL, test_data_AAPL should be Pandas DataFrames

try:
    X_train_AAPL, y_train_AAPL = prepare_sequences(train_data_AAPL, n_past)
    X_test_AAPL, y_test_AAPL = prepare_sequences(test_data_AAPL, n_past)

    X_train_AMD, y_train_AMD = prepare_sequences(train_data_AMD, n_past)
    X_test_AMD, y_test_AMD = prepare_sequences(test_data_AMD, n_past)

    X_train_GOOGL, y_train_GOOGL = prepare_sequences(train_data_GOOGL, n_past)
    X_test_GOOGL, y_test_GOOGL = prepare_sequences(test_data_GOOGL, n_past)

    X_train_NVDA, y_train_NVDA = prepare_sequences(train_data_NVDA, n_past)
    X_test_NVDA, y_test_NVDA = prepare_sequences(test_data_NVDA, n_past)

    X_train_TEAM, y_train_TEAM = prepare_sequences(train_data_TEAM, n_past)
    X_test_TEAM, y_test_TEAM = prepare_sequences(test_data_TEAM, n_past)
except ValueError as e:
    print(e)

# Define a list of stock symbols and their corresponding data
stocks = [("AAPL", X_train_AAPL, y_train_AAPL, X_test_AAPL, y_test_AAPL),
          ("AMD", X_train_AMD, y_train_AMD, X_test_AMD, y_test_AMD),
          ("GOOGL", X_train_GOOGL, y_train_GOOGL, X_test_GOOGL, y_test_GOOGL),
          ("NVDA", X_train_NVDA, y_train_NVDA, X_test_NVDA, y_test_NVDA),
          ("TEAM", X_train_TEAM, y_train_TEAM, X_test_TEAM, y_test_TEAM)]

# Iterate through the list of stocks and print the set sizes
for stock, X_train, y_train, X_test, y_test in stocks:
    print(f"Training set size - {stock}:-")
    print(X_train.shape, y_train.shape)
    print("\n")
    print(f"Testing set size - {stock}:-")
    print(X_test.shape, y_test.shape)
    print("\n")

# Create a list of stock symbols and their corresponding X_train and X_test data
stocks_data_reshape = [("AAPL", X_train_AAPL, X_test_AAPL),
               ("AMD", X_train_AMD, X_test_AMD),
               ("GOOGL", X_train_GOOGL, X_test_GOOGL),
               ("NVDA", X_train_NVDA, X_test_NVDA),
               ("TEAM", X_train_TEAM, X_test_TEAM)]

# Reshape input data for each stock
for stock, X_train, X_test in stocks_data_reshape:
    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

    print(f"Reshaped input data for {stock} - Training set size: {X_train.shape}, Testing set size: {X_test.shape}")

# Function to initialize a sequential model with LSTM layers
def initialize_lstm_model(input_shape=(X_train_AAPL.shape[1], 1)):
# Initialize a sequential model
    model = Sequential()
# First LSTM layer with 50 units, input shape, and return sequences
    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))
    model.add(Dropout(0.2))
# Second LSTM layer with 50 units and return sequences
    model.add(LSTM(units=50, return_sequences=True))
    model.add(Dropout(0.2))
# Third LSTM layer with 50 units
    model.add(LSTM(units=50))
    model.add(Dropout(0.2))
# Add a dense output layer with one uniT
    model.add(Dense(units=1))
    return model

# Initialize models for each stock
model_AAPL = initialize_lstm_model()
model_AMD = initialize_lstm_model()
model_GOOGL = initialize_lstm_model()
model_NVDA = initialize_lstm_model()
model_TEAM = initialize_lstm_model()

model_AAPL.summary()
model_AMD.summary()
model_GOOGL.summary()
model_NVDA.summary()
model_TEAM.summary()

models = [model_AAPL, model_AMD, model_GOOGL, model_NVDA, model_TEAM]
for model in models:
    model.compile(loss='mean_squared_error', optimizer='adam')

i =[]
# Define your callbacks (outside the loop)
checkpoints = ModelCheckpoint(filepath=f'weights_stock_{i}.h5', save_best_only=True)
early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)

# Define the number of epochs and batch size
epochs = 250
batch_size = 32

# Create a list of models for each stock
models = [model_AAPL, model_AMD, model_GOOGL, model_NVDA, model_TEAM]

# Create a list of training and testing data for each stock
training_data = [(X_train_AAPL, y_train_AAPL, X_test_AAPL, y_test_AAPL),
                 (X_train_AMD, y_train_AMD, X_test_AMD, y_test_AMD),
                 (X_train_GOOGL, y_train_GOOGL, X_test_GOOGL, y_test_GOOGL),
                 (X_train_NVDA, y_train_NVDA, X_test_NVDA, y_test_NVDA),
                 (X_train_TEAM, y_train_TEAM, X_test_TEAM, y_test_TEAM)]

# Loop through the stocks and train each model
for i, (X_train, y_train, X_test, y_test) in enumerate(training_data):
    model = models[i]  # Get the corresponding model

    # Training the model for the current stock
    model.fit(X_train, y_train,
              validation_data=(X_test, y_test),
              epochs=epochs,
              batch_size=batch_size,
              verbose=1,
              callbacks=[checkpoints, early_stopping])

# Create a list of models for each stock
models = [model_AAPL, model_AMD, model_GOOGL, model_NVDA, model_TEAM]

# Create a list of training and testing data for each stock
training_data = [(X_train_AAPL, y_train_AAPL, X_test_AAPL, y_test_AAPL),
                 (X_train_AMD, y_train_AMD, X_test_AMD, y_test_AMD),
                 (X_train_GOOGL, y_train_GOOGL, X_test_GOOGL, y_test_GOOGL),
                 (X_train_NVDA, y_train_NVDA, X_test_NVDA, y_test_NVDA),
                 (X_train_TEAM, y_train_TEAM, X_test_TEAM, y_test_TEAM)]

# Initialize lists to store performance metrics
train_rmse = []
test_rmse = []

train_predictions_list = []
test_predictions_list = []

# Loop through the stocks and evaluate each model
for i, (X_train, y_train, X_test, y_test) in enumerate(training_data):
    model = models[i]  # Get the corresponding model

    # Make predictions on the training and testing data
    train_predictions = model.predict(X_train)
    test_predictions = model.predict(X_test)

    # Store predictions
    train_predictions_list.append(train_predictions)
    test_predictions_list.append(test_predictions)

    # Calculate RMSE for training and testing data
    train_rmse.append(math.sqrt(mean_squared_error(y_train, train_predictions)))
    test_rmse.append(math.sqrt(mean_squared_error(y_test, test_predictions)))

# Display the RMSE for each stock
for i, (stock, _, _, _, _) in enumerate(stocks):
    print(f"RMSE for {stock} - Training: {train_rmse[i]}, Testing: {test_rmse[i]}")

print(train_predictions_list)

# Set the number of previous time steps to consider for plotting
look_back = 60

# Initialize an array for plotting the train predictions
trainPredictPlot = np.empty_like(new_datasetAAPL)
trainPredictPlot[:] = np.nan
# Assign the predicted values to the appropriate location for train predictions
trainPredictPlot[look_back:len(train_predictions_list[4])+look_back] = train_predictions_list[4].flatten()

# Initialize an array for plotting the test predictions
testPredictPlot = np.empty_like(new_datasetAAPL)
testPredictPlot[:] = np.nan
# Calculate the starting index for the test predictions
test_start = len(new_datasetAAPL) - len(test_predictions_list[4])
# Assign the predicted values to the appropriate location for test predictions
testPredictPlot[test_start:] = test_predictions_list[4].flatten()

# Rescale the scaled data back to its original scale using the scaler
original_scaled_data = scaler.inverse_transform(scaled_datasetAAPL)

# Plotting the baseline data, training predictions, and test predictions
plt.figure(figsize=(15, 6))
plt.plot(original_scaled_data, color='black', label=f"Actual AAPL price")
plt.plot(trainPredictPlot, color='red', label=f"Predicted AAPL price(train set)")
plt.plot(testPredictPlot, color='blue', label=f"Predicted AAPL price(test set)")

plt.title(f"AAPL share price")
plt.xlabel("time")
plt.ylabel(f"AAPL share price")
plt.legend()
plt.show()